{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your project, where you aim to explore maps using a robot, **Deep Q-Networks (DQN)** is typically a more suitable choice compared to traditional **Q-learning**. Here’s why:\n",
    "\n",
    "### Why DQN is More Suitable:\n",
    "\n",
    "- **Q-learning** works well for environments with a **small and discrete state space**, where you can represent the state-action values in a Q-table.\n",
    "- However, in your case, the **state space** (the robot's Field of View, position, map, etc.) is likely too large and complex for a Q-table to handle efficiently.\n",
    "- **DQN** addresses this by using a **neural network** to approximate the Q-values, making it suitable for environments with **large or continuous state spaces**, such as your robot’s exploration map with complex sensor inputs.\n",
    "\n",
    "### Step-by-Step Guide: Applying DQN to Your Project\n",
    "\n",
    "Here’s how to apply **DQN** step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Setup the Environment (dummy_gym)**\n",
    "\n",
    "Your environment, `dummy_gym`, should provide:\n",
    "- **State (observation space)**: The robot’s Field of View (FOV), position, and obstacles.\n",
    "- **Action space**: Discrete actions such as moving up, down, left, or right.\n",
    "- **Reward**: The reward structure you defined earlier (e.g., small penalties for moving, rewards for exploring new areas, etc.).\n",
    "\n",
    "Make sure `dummy_gym` is set up to provide these. You can continue with the setup you've already created for this environment.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Define the DQN Model**\n",
    "\n",
    "DQN uses a neural network to approximate the Q-values for each state-action pair.\n",
    "\n",
    "Here’s a simple DQN model using **PyTorch**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 128)  # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(128, output_size)  # Output layer for Q-values\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Output the Q-values for each action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **input_size**: The size of the flattened observation (e.g., the robot's FOV matrix and other state information).\n",
    "- **output_size**: The number of possible actions (e.g., 4 for up, down, left, right).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. **Define the Replay Buffer**\n",
    "\n",
    "DQN uses a **replay buffer** to store experiences (state, action, reward, next state, done) and train the network by sampling from this buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 4. **Training Loop for DQN**\n",
    "\n",
    "You will now define the main training loop using the DQN model. The steps involved are:\n",
    "\n",
    "1. **Initialize environment and model**.\n",
    "2. **Take actions** using an epsilon-greedy policy (explore vs exploit).\n",
    "3. **Store transitions** (state, action, reward, next state) in the replay buffer.\n",
    "4. **Sample a batch** from the replay buffer and train the model.\n",
    "5. **Update the target network** periodically.\n",
    "\n",
    "Here’s the implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon, model, action_size):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_size - 1)  # Explore\n",
    "    else:\n",
    "        state = torch.tensor(state, dtype=torch.float32).flatten().unsqueeze(0)\n",
    "        q_values = model(state)\n",
    "        return q_values.argmax().item()  # Exploit\n",
    "\n",
    "def train_dqn(env, dqn_model, target_model, replay_buffer, optimizer, batch_size=32, gamma=0.99):\n",
    "    if replay_buffer.size() < batch_size:\n",
    "        return\n",
    "    \n",
    "    # Sample from replay buffer\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    states = torch.tensor(states, dtype=torch.float32).flatten(1)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32).flatten(1)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    # Get the current Q-values\n",
    "    q_values = dqn_model(states).gather(1, actions)\n",
    "    \n",
    "    # Get the target Q-values\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_model(next_states).max(1, keepdim=True)[0]\n",
    "        target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = nn.MSELoss()(q_values, target_q_values)\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 1000\n",
    "epsilon = 1.0  # Start with exploration\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.99  # Discount factor\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "target_update = 10  # How often to update the target network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dummy_gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-af1f8d30591e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize the environment and models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdummy_gym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyGym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyGym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_obstacles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFOV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m  \u001b[0;31m# Action space (up, down, left, right)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dummy_gym'"
     ]
    }
   ],
   "source": [
    "# Initialize the environment and models\n",
    "from dummy_gym import DummyGym\n",
    "env = DummyGym(init_pos=(2, 3), map_size=(30, 30), num_of_obstacles=140, FOV=(5, 5))\n",
    "input_size = env.observation_space().size\n",
    "output_size = 4  # Action space (up, down, left, right)\n",
    "\n",
    "dqn_model = DQN(input_size, output_size)\n",
    "target_model = DQN(input_size, output_size)\n",
    "target_model.load_state_dict(dqn_model.state_dict())  # Sync target model\n",
    "\n",
    "optimizer = optim.Adam(dqn_model.parameters(), lr=learning_rate)\n",
    "replay_buffer = ReplayBuffer(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(state, epsilon, dqn_model, output_size)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        train_dqn(env, dqn_model, target_model, replay_buffer, optimizer, batch_size, gamma)\n",
    "    \n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    # Update target model every few episodes\n",
    "    if episode % target_update == 0:\n",
    "        target_model.load_state_dict(dqn_model.state_dict())\n",
    "    \n",
    "    print(f\"Episode {episode} - Total Reward: {total_reward}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Explanation of Key Components:**\n",
    "\n",
    "1. **Epsilon-Greedy Policy**: This balances exploration (choosing random actions) and exploitation (choosing actions based on the learned Q-values).\n",
    "   - Start with high exploration (`epsilon = 1.0`), and decay it over time to favor exploitation.\n",
    "   \n",
    "2. **Replay Buffer**: Stores transitions `(state, action, reward, next_state, done)` and helps break correlation between consecutive experiences.\n",
    "\n",
    "3. **Q-value Update**: \n",
    "   - **Q-value**: Q(s, a) is the expected future reward of taking action `a` in state `s`.\n",
    "   - **Target Q-value**: The target Q-value is the sum of the immediate reward and the discounted maximum Q-value of the next state:  \n",
    "     $\n",
    "     Q_{\\text{target}} = r + \\gamma \\max_{a'} Q_{\\text{next}}(s', a')\n",
    "     $\n",
    "\n",
    "4. **Target Network**: DQN uses a **target network** (a copy of the main network) that is updated less frequently. This stabilizes training by preventing the target Q-values from changing too rapidly.\n",
    "\n",
    "5. **Training the Model**: The model is trained using the loss function, which minimizes the difference between the current Q-values and the target Q-values.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **Why DQN?**: DQN is more suitable than traditional Q-learning because it can handle large state spaces by approximating Q-values using a neural network.\n",
    "- **Replay Buffer**: Helps in stabilizing training by reusing past experiences.\n",
    "- **Target Network**: Reduces oscillations during training by keeping the target values more stable.\n",
    "- **Training Loop**: Involves sampling from the replay buffer and training the model using backpropagation on the Q-value loss.\n",
    "\n",
    "By following this process, you can successfully apply DQN to your robot exploration project, allowing the robot to efficiently learn exploration strategies through interaction with the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
