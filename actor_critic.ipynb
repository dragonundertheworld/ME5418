{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply **Actor-Critic** methods to your project, you can combine the strengths of **policy-gradient methods** (actor) and **value-based methods** (critic). In an Actor-Critic framework, you have two neural networks:\n",
    "\n",
    "1. **Actor**: Learns a policy and decides which action to take based on the current state.\n",
    "2. **Critic**: Evaluates the value of the current state (or state-action pair) and provides feedback to improve the actor's performance.\n",
    "\n",
    "Here’s a step-by-step guide on how to apply Actor-Critic to your project:\n",
    "\n",
    "### Steps to Apply Actor-Critic:\n",
    "\n",
    "1. **Define the Actor Network**: Outputs a probability distribution over actions (like in REINFORCE).\n",
    "2. **Define the Critic Network**: Outputs a value for the current state (or state-action pair).\n",
    "3. **Interaction with the Environment**: Use the actor to sample actions, and the critic to estimate the value of states.\n",
    "4. **Update Both Networks**: The actor is updated using the policy gradient, and the critic is updated to minimize the difference between the predicted and actual value (TD-error).\n",
    "\n",
    "### 1. Define the Actor-Critic Networks\n",
    "\n",
    "You will need two neural networks: one for the **Actor** and one for the **Critic**. Both networks can be simple fully connected networks.\n",
    "\n",
    "#### Actor Network (Policy):\n",
    "The actor outputs the probability distribution over actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhihan/ME5418\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "# test command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=-1)  # Output a probability distribution\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Critic Network (Value):\n",
    "The critic outputs a value (scalar) for the given state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)  # Outputs a single value\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        value = self.fc2(x)  # Outputs state value\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Select Actions and Get Value Estimates\n",
    "\n",
    "You need to use both the actor (to sample actions) and the critic (to estimate the value of the current state). Here's an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(actor, state):\n",
    "    state = torch.tensor(state, dtype=torch.float32).flatten()  # Flatten the state (FOV)\n",
    "    action_probs = actor(state)\n",
    "    action_distribution = torch.distributions.Categorical(action_probs)\n",
    "    action = action_distribution.sample()  # Sample an action\n",
    "    return action.item(), action_distribution.log_prob(action)\n",
    "\n",
    "def get_value(critic, state):\n",
    "    state = torch.tensor(state, dtype=torch.float32).flatten()  # Flatten the state\n",
    "    return critic(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Collect Trajectories and Compute Returns\n",
    "\n",
    "You’ll need to interact with the environment and collect states, actions, rewards, and value estimates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(env, actor, critic, max_steps=1000):\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    for step in range(max_steps):\n",
    "        action, log_prob = select_action(actor, state)\n",
    "        value = get_value(critic, state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return log_probs, values, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Compute Advantage and Update Actor-Critic\n",
    "\n",
    "In the Actor-Critic method, the **advantage** is the difference between the actual reward and the value estimated by the critic. You’ll use this advantage to update the actor and critic:\n",
    "\n",
    "- **Advantage**: `A(s, a) = R - V(s)` (where `R` is the actual return, and `V(s)` is the critic's estimate of the state value).\n",
    "\n",
    "You can compute the **returns** and **advantages** like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def update_actor_critic(actor, critic, optimizer_actor, optimizer_critic, log_probs, values, rewards, gamma=0.99):\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    \n",
    "    values = torch.cat(values)\n",
    "    \n",
    "    # Advantage is the difference between the return and the value\n",
    "    advantages = returns - values\n",
    "    \n",
    "    # Actor loss\n",
    "    actor_loss = 0\n",
    "    for log_prob, advantage in zip(log_probs, advantages):\n",
    "        actor_loss -= log_prob * advantage  # Policy gradient update\n",
    "    \n",
    "    # Critic loss (mean squared error between predicted value and return)\n",
    "    critic_loss = torch.mean((returns - values) ** 2)\n",
    "    \n",
    "    # Update actor\n",
    "    optimizer_actor.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    optimizer_actor.step()\n",
    "    \n",
    "    # Update critic\n",
    "    optimizer_critic.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    optimizer_critic.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Actor Loss**: The actor loss is computed based on the advantage (i.e., how much better or worse the action was compared to the expected value).\n",
    "- **Critic Loss**: The critic is updated by minimizing the difference between the actual return and the value estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training Loop\n",
    "\n",
    "Now integrate everything into the training loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' must be tuple of ints, but found element of type builtin_function_or_method at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7670b6e90c37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define the networks and optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCriticNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bff418577704>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActorNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/me5418-SAPP/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: empty(): argument 'size' must be tuple of ints, but found element of type builtin_function_or_method at pos 2"
     ]
    }
   ],
   "source": [
    "\n",
    "from dummy_gym import DummyGym\n",
    "env = DummyGym(init_pos=(2, 3), map_size=(30, 30), num_of_obstacles=140, FOV=(5, 5))\n",
    "\n",
    "# Define the networks and optimizers\n",
    "actor = ActorNetwork(input_size=env.observation_space().count, output_size=4)\n",
    "critic = CriticNetwork(input_size=env.observation_space().count)\n",
    "\n",
    "optimizer_actor = torch.optim.Adam(actor.parameters(), lr=1e-3)\n",
    "optimizer_critic = torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "for episode in range(1000):\n",
    "    log_probs, values, rewards = collect_trajectory(env, actor, critic)\n",
    "    update_actor_critic(actor, critic, optimizer_actor, optimizer_critic, log_probs, values, rewards)\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Key Concepts Recap:\n",
    "- **Actor Network**: Outputs the probability distribution over actions.\n",
    "- **Critic Network**: Outputs a value estimate for the current state.\n",
    "- **Advantage Calculation**: The advantage is the difference between the actual return and the value estimate, used to guide the policy updates.\n",
    "- **Updates**: The actor is updated using the policy gradient, while the critic is updated by minimizing the value estimation error.\n",
    "\n",
    "### Benefits of Actor-Critic:\n",
    "- **Stabilization**: The critic helps stabilize the learning process by providing better estimates of the expected return, reducing variance in policy updates.\n",
    "- **Continuous Learning**: Actor-Critic methods can be extended to handle continuous action spaces and more complex tasks.\n",
    "\n",
    "This framework will allow your robot to learn more efficiently by combining the exploration strengths of policy-gradient methods with the value estimation of critic-based methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
