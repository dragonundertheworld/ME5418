{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import modules and run an example of dummy_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 15:00:24.600827: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 15:00:25.969606: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib\n",
      "2024-11-22 15:00:25.969727: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib\n",
      "2024-11-22 15:00:25.969735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-11-22 15:00:27.530928: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-11-22 15:00:27.530963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Legion): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from dummy_gym import *\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# if there is a GPU available, set train the device to GPU\n",
    "device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Only I need to do this because I am running the code on my local machine\n",
    "# os.chdir('/home/zhihan/ME5418')\n",
    "\n",
    "# Create the environment instance\n",
    "env = DummyGym()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters setting\n",
    "In order to test our model, we did not assign large parameters. Set the training mode using one of the following options:\n",
    "### Training Mode Options\n",
    "- **`train`**: Start training from scratch, overwriting existing model, state, and replay buffer files.\n",
    "- **`test`**: Test the model without overwriting any files.\n",
    "- **`train_from_breakpoint`**: Continue training from the last saved breakpoint.\n",
    "- **`fine_tune`**: Fine-tune the model while preserving the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_size is 4\n"
     ]
    }
   ],
   "source": [
    "# 断点文件会保存在train_breakpoint文件夹中\n",
    "# 如果中途训练中断，可以从断点处继续训练，只需将train_mode设置为'train_from_breakpoint'，包括模型文件、状态文件和replay_buffer文件\n",
    "# 如果想重新训练，将train_mode设置为'train'即可，模型文件、状态文件和replay_buffer文件会被覆盖\n",
    "# 如果想测试模型，将train_mode设置为'test'即可，模型文件、状态文件和replay_buffer文件不会被覆盖\n",
    "# 如果既想重新训练，又想保留之前的模型，即微调模型，将train_mode设置为'fine_tune'即可\n",
    "\n",
    "train_mode = 'test'\n",
    "# train_mode = 'train' # 'train' or 'test' or 'train_from_breakpoint' or 'fine_tune'\n",
    "model_name = 'dqn_model.h5'\n",
    "state_name = 'state.npz'\n",
    "replay_buffer_name = 'replay_buffer.pkl'\n",
    "\n",
    "breakpoint_path = 'train_breakpoint'\n",
    "model_path = os.path.join(breakpoint_path, model_name)\n",
    "state_path = os.path.join(breakpoint_path, state_name)\n",
    "replay_buffer_path = os.path.join(breakpoint_path, replay_buffer_name)\n",
    "\n",
    "# Hyperparameters\n",
    "original_state = env.observe()\n",
    "action_size = env.action_space.n\n",
    "print(f'action_size is {action_size}')\n",
    "batch_size = 32 # 每次训练的数据量\n",
    "n_episodes = 5 # 地图数量\n",
    "time_steps = 1000 # 每张地图最多走多少步，避免陷入局部重复导致训练无效数据\n",
    "epochs = 10 # 每个地图训练次数\n",
    "\n",
    "# DQN parameters\n",
    "gamma = 0.99  # discount rate, 0.99 means the future rewards are considered important\n",
    "epsilon = 0.6 # 探索率\n",
    "epsilon_min = 0.01 # 最小探索率\n",
    "epsilon_decay = 0.995 # 探索衰减率\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer = deque(maxlen=2000) # 用于存储训练数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing states\n",
    "The state we get from dummy_gym are lists and have only 2 dimensions. So we have to\n",
    "1. Use `np.array` to change the type of state to `ndarray` for future data processing\n",
    "2. Use `map.reshape((1, *map.shape, 1))` to reshape the state to 4 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理原始state\n",
    "def prepare_state(original_state):\n",
    "    '''\n",
    "    Prepare the state for the model by numpying and adding dimension\n",
    "    '''\n",
    "    new_state = []\n",
    "    new_state_shape = []\n",
    "    for state in original_state:\n",
    "        state = np.array(state)\n",
    "        # add dimension\n",
    "        state = state.reshape(1, *state.shape, 1) # (30, 30) -> (1, 30, 30, 1)\n",
    "        new_state.append(state)\n",
    "        new_state_shape.append(state.shape)\n",
    "\n",
    "    return new_state, new_state_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, state_shape = prepare_state(original_state)\n",
    "state_shape[0][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Designing out model\n",
    "After comparing different model output, we found that model behaves best when follows these settings:\n",
    "1. `pool=True`\n",
    "2. `dropout=True`\n",
    "3. `residual=True`\n",
    "4. `batch_norm=True`\n",
    "5. `regularizer=None`\n",
    "\n",
    "You can see the summay of the model in below ceil.\\\n",
    "**Model is loaded from `model_path` instead of being created if `train_mode` is set `fine_tune` or `train_from_breakpoint`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 15:00:27.585775: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_visit_count_state (Input  [(None, 30, 30, 1)]  0          []                               \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_fov_map (InputLayer)     [(None, 10, 10, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 30, 30, 32)   320         ['input_visit_count_state[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 10, 10, 16)   160         ['input_fov_map[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 30, 30, 32)   9248        ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 10, 10, 16)   2320        ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 30, 30, 32)  128         ['conv2d_1[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 10, 10, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 30, 30, 32)   0           ['input_visit_count_state[0][0]',\n",
      "                                                                  'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 10, 10, 16)   0           ['input_fov_map[0][0]',          \n",
      "                                                                  'batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 30, 30, 32)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 10, 10, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 15, 15, 32)   0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 5, 5, 16)    0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " input_car_pos (InputLayer)     [(None, 2, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 7200)         0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 400)          0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 2)            0           ['input_car_pos[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 7602)         0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           486592      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           2080        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4)            132         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 501,044\n",
      "Trainable params: 500,948\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_dqn_model(conv_filters_1=32, conv_filters_2=16, kernel_size=(3,3), activation='sigmoid', \n",
    "                     dense_units_1=64, dense_units_2=32, pool_size=(2,2), padding='same',\n",
    "                     pool=True, dropout=True, residual=True, batch_norm=True, regularizer=None, LSTM=False, flat=True, dense_twice=True): # 多一个全连接多2000参数\n",
    "                                                                                                                                           # 如果flat=False参数直接减少183万，但会丢失空间信息，因为GlobalAveragePooling2D会平均所有值\n",
    "    # Input for state_array[0].shape array(120*120)\n",
    "    input_visit_count_state = tf.keras.layers.Input(shape=state_shape[0][1:], name='input_visit_count_state')\n",
    "    conv_visit_count_state = tf.keras.layers.Conv2D(conv_filters_1, kernel_size, activation=activation, padding=padding)(input_visit_count_state)\n",
    "    conv_visit_count_state = tf.keras.layers.Conv2D(conv_filters_1, kernel_size, activation=activation, padding=padding)(conv_visit_count_state)\n",
    "    # print(conv_visit_count_state.shape)\n",
    "\n",
    "    # different settings    \n",
    "    batch_norm_visit_count_state = tf.keras.layers.BatchNormalization()(conv_visit_count_state) if batch_norm else conv_visit_count_state\n",
    "    residual_visit_count_state = tf.keras.layers.Add()([input_visit_count_state, batch_norm_visit_count_state])     if residual else batch_norm_visit_count_state\n",
    "    dropout_visit_count_state  = tf.keras.layers.Dropout(0.2)(residual_visit_count_state)                     if dropout  else residual_visit_count_state\n",
    "    pool_visit_count_state     = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(dropout_visit_count_state) if pool     else dropout_visit_count_state\n",
    "    \n",
    "    flat_visit_count_state = tf.keras.layers.Flatten()(pool_visit_count_state) if flat == True else tf.keras.layers.GlobalAveragePooling2D()(pool_visit_count_state)\n",
    "\n",
    "    # Input for state_array[1].shape 10x10 array\n",
    "    input_fov_map = tf.keras.layers.Input(shape=state_shape[1][1:], name='input_fov_map')\n",
    "    conv_fov_map = tf.keras.layers.Conv2D(conv_filters_2, kernel_size, activation=activation, padding=padding)(input_fov_map)\n",
    "    conv_fov_map = tf.keras.layers.Conv2D(conv_filters_2, kernel_size, activation=activation, padding=padding)(conv_fov_map)\n",
    "    # print(conv_fov_map.shape)\n",
    "\n",
    "    # different settings\n",
    "    batch_norm_fov_map = tf.keras.layers.BatchNormalization()(conv_fov_map) if batch_norm else conv_fov_map\n",
    "    residual_fov_map = tf.keras.layers.Add()([input_fov_map, batch_norm_fov_map]) if residual else batch_norm_fov_map\n",
    "    dropout_fov_map_state = tf.keras.layers.Dropout(0.2)(residual_fov_map) if dropout else residual_fov_map\n",
    "    pool_fov_map_state = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(dropout_fov_map_state) if pool else dropout_fov_map_state\n",
    "    \n",
    "    flat_fov_map = tf.keras.layers.Flatten()(pool_fov_map_state) if flat == True else tf.keras.layers.GlobalAveragePooling2D()(pool_fov_map_state)\n",
    "\n",
    "    # Input for 2x1 array\n",
    "    input_car_pos = tf.keras.layers.Input(shape=state_shape[2][1:], name='input_car_pos')\n",
    "    dense_car_pos = tf.keras.layers.Flatten()(input_car_pos)\n",
    "\n",
    "    # Concatenate all branches\n",
    "    combined = tf.keras.layers.Concatenate()([flat_visit_count_state, flat_fov_map, dense_car_pos])\n",
    "\n",
    "    # Add LSTM layer for sequential processing\n",
    "    lstm_layer = tf.keras.layers.Reshape((1, combined.shape[1]))(combined)  # Reshape for LSTM input\n",
    "    lstm_layer = tf.keras.layers.LSTM(64, activation=activation)(lstm_layer)  # LSTM layer added\n",
    "\n",
    "    # Fully connected layers after concatenation\n",
    "    dense1 = tf.keras.layers.Dense(dense_units_1, activation=activation, kernel_regularizer=regularizer)(lstm_layer if LSTM else combined)\n",
    "    dense2 = tf.keras.layers.Dense(dense_units_2, activation=activation, kernel_regularizer=regularizer)(dense1) if dense_twice else dense1\n",
    "    output = tf.keras.layers.Dense(action_size, activation='linear', kernel_regularizer=regularizer)(dense2)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[input_visit_count_state, input_fov_map, input_car_pos], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the DQN model and target model\n",
    "if train_mode == 'train_from_breakpoint' or train_mode == 'fine_tune' or train_mode == 'test':\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    target_model = tf.keras.models.load_model(model_path)\n",
    "elif train_mode == 'train':\n",
    "    model = create_dqn_model()\n",
    "    target_model = create_dqn_model()\n",
    "    target_model.set_weights(model.get_weights())\n",
    "else:\n",
    "    raise ValueError('train_mode should be either train, test, fine_tune or train_from_breakpoint')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select actions\n",
    "Actions are selected randomly with the possibility of epsilon. Otherwise will be selected based on `q_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon):\n",
    "    '''\n",
    "    Select action using epsilon-greedy policy\n",
    "    '''\n",
    "    if np.random.rand() <= epsilon:\n",
    "        action = random.randrange(action_size)  # Explore: random action\n",
    "        print(f'Ready to take random action:{action}')\n",
    "    else:\n",
    "#         env.render(map_type=\"visit_count\")\n",
    "        q_values = model.predict(state)  # Exploit: select action with max Q-value\n",
    "        action_list = np.argsort(q_values[0])[::-1]\n",
    "        action = action_list[0]\n",
    "        print(f'Ready to take optimal action:{action} with q_values:{q_values}')\n",
    "    \n",
    "    return action  # Exploit: select action with max Q-value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define function of trainnign model\n",
    "1. **Sample a Minibatch**:\n",
    "   - A minibatch of experiences is randomly sampled from the `replay_buffer` to train the model. Each experience is in the format `(state, action, reward, next_state, done)`.\n",
    "\n",
    "2. **Loop through each Experience**:\n",
    "   - The code iterates through each experience in the minibatch to update the model. \n",
    "\n",
    "3. **Initialize Target**:\n",
    "   - The immediate reward is set as the initial target.\n",
    "\n",
    "4. **Adjust Batch Dimension**:\n",
    "   - This step ensures that the state tensors have a batch dimension (shape of 4D) by expanding dimensions if needed.\n",
    "\n",
    "5. **Compute Target if Episode Not Done**:\n",
    "   - If the episode is not finished (`done` is `False`), the target Q-value is updated using the formula:\\\n",
    "     $\n",
    "     \\text{target} = \\text{reward} + \\gamma \\cdot \\max_{a'} Q_{\\text{target_model}}(\\text{next_state}, a')\n",
    "     $\n",
    "   - `gamma` is the discount factor that weighs the contribution of future rewards. The target model is used to predict the Q-values of the next state.\n",
    "\n",
    "6. **Update Predicted Q-Values**:\n",
    "   - The current state's Q-values are predicted, and the Q-value for the action taken is updated with the calculated target value.\n",
    "\n",
    "7. **Train the Model**:\n",
    "   - The model is trained on the current state and updated Q-values for `epochs` iterations. The loss for each training step is stored in `history`.\n",
    "\n",
    "8. **Plot Loss**:\n",
    "   - This section plots the loss curve for the training steps within this minibatch.\n",
    "\n",
    "9. **Update Target Model Weights**:\n",
    "   - After training, the weights of the target model are updated to match the main model. This helps stabilize training by having a slightly outdated model (target model) to compare predictions.\n",
    "\n",
    "10. **Return the Losses**:\n",
    "    - The function returns the loss values for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(batch_size): # learning agent\n",
    "    with tf.device(device):\n",
    "        minibatch = random.sample(replay_buffer, batch_size) # Sample minibatch from the replay buffer\n",
    "        for state, action, reward, next_state, done in minibatch: # 从小批量样本中获取数据,32*avg_loss_decrease=64\n",
    "            target = reward + gamma * np.amax(target_model.predict(next_state,verbose=0)[0])# 当前状态的reward作为target\n",
    "            target_q_values = target_model.predict(state,verbose=0) # 得到4个动作的Q值\n",
    "            target_q_values[0][action] = target # 将对应动作的Q值更新为target\n",
    "            \n",
    "            history = model.fit(state, target_q_values, epochs=epochs, verbose=0) # 对这个小批量样本训练epochs次,verbose=1表示显示训练进度条\n",
    "\n",
    "        # Update target model weights\n",
    "        target_model.set_weights(model.get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start tranning!\n",
    "1. **Define saving and loading breakpoint function**:\n",
    "   - In case of emergency like sudden death of kernel, function of saving and recovery should be defined.\n",
    "\n",
    "2. **Loop Over Episodes**:\n",
    "   - Iterates through a set number of episodes (`n_episodes`), each representing a complete run of the environment.\n",
    "\n",
    "3. **Environment Reset**:\n",
    "   - Resets the environment at the start of each episode and prepares the initial state.\n",
    "\n",
    "4. **Loop Over Time Steps**:\n",
    "   - Within each episode, it loops through a maximum number of time steps (`time_steps`), representing agent actions within the environment.\n",
    "\n",
    "5. **Select and Execute an Action**:\n",
    "   - Selects an action based on the current policy (using an epsilon-greedy strategy) and takes a step in the environment to obtain the next state, reward, and done flag.\n",
    "\n",
    "6. **Render and Prepare Next State**:\n",
    "   - Renders the environment (to visualize changes) and processes the next state for input into the network.\n",
    "\n",
    "7. **Store Experience**:\n",
    "   - The current experience `(state, action, reward, next_state, done)` is added to the replay buffer to be used later for training.\n",
    "\n",
    "8. **Update Rewards**:\n",
    "   - Adds the reward received at the current time step to the total reward for the episode.\n",
    "\n",
    "9. **Check if Episode is Complete**:\n",
    "   - Ends the episode if the `done` flag is set to `True` and prints the episode's result.\n",
    "\n",
    "10. **Train the Model if Replay Buffer is Large Enough**:\n",
    "    - If the replay buffer contains enough samples, the DQN model is trained using a batch of experiences.\n",
    "\n",
    "11. **Store Total Reward and Update Epsilon**:\n",
    "    - Appends the total reward for the episode to `rewards_list` and decays epsilon to gradually reduce exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_breakpoint():\n",
    "    model.save(model_path)\n",
    "    env.save_state(state_path)\n",
    "    with open(replay_buffer_path, 'wb') as f:\n",
    "        pickle.dump(replay_buffer, f)\n",
    "\n",
    "def load_breakpoint():\n",
    "    # Model already loaded\n",
    "    with open(replay_buffer_path, 'rb') as f:\n",
    "        replay_buffer = pickle.load(f)\n",
    "    env.load_state(state_path)\n",
    "    return replay_buffer\n",
    "\n",
    "if train_mode=='train_from_breakpoint':\n",
    "    replay_buffer = load_breakpoint()\n",
    "    print('Loading model and replay buffer from breakpoint')\n",
    "    env.render(map_type='visit_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if train_mode != 'test':\n",
    "    for episode in range(n_episodes): # 训练多少张图\n",
    "        print(f\"\\n********************Episode: {episode+1}/{n_episodes}********************\")\n",
    "\n",
    "        # Reset the environment and get the initial state\n",
    "        state = env.state if train_mode== 'train_from_breakpoint' else env.reset()\n",
    "        state,_ = prepare_state(state)\n",
    "        epsilon = epsilon * epsilon_decay if epsilon > epsilon_min else epsilon_min\n",
    "        replay_buffer = deque(maxlen=2000)\n",
    "        images = []\n",
    "\n",
    "        # Loop through each time step\n",
    "        for time_step in range(time_steps): \n",
    "            print(f\"\\n-------------------Time step: {time_step}------------------------\")\n",
    "            images.append(env.visit_count / np.max(env.visit_count) * 255)\n",
    "            # In each time step, agent selects an action and implement it, environment provides feedback\n",
    "            action = select_action(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                env.render(map_type=\"visit_count\")\n",
    "                save_to_gif(images,'dqn_gif', f'policy_by_DQN_{time.strftime(\"%m-%d_%H-%M\")}.gif')\n",
    "                print(\"This eqisode is done, ready for next one.\")\n",
    "                break\n",
    "            cells_visited = state[0][state[0] == EXPLORED].shape[0] # 计算已经访问的cell数量\n",
    "            cells = env.map_size[0]*env.map_size[1]\n",
    "            print(f\"Cells visited: {cells_visited}/{env.map_size[0]*env.map_size[1]} at time step: {time_step}\")\n",
    "            print(f\"Reward after this action:{reward}\")\n",
    "\n",
    "            # Store the data into replay buffer\n",
    "            next_state,_ = prepare_state(next_state)\n",
    "            replay_buffer.append((state, action, reward, next_state, done)) # 将数据存入replay buffer\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "            # Train the model when replay buffer is larger than batch size\n",
    "            if len(replay_buffer) > batch_size: # 当步数大于batch size时开始训练\n",
    "                print(f\"Start training DQN model with batch size: {batch_size}\")\n",
    "                losses = train_dqn(batch_size)\n",
    "\n",
    "                # If we are in train mode, save the gym, model and replay buffer for emergency stop every 100 time steps\n",
    "                if (train_mode == 'train' or train_mode == 'fine_tune') and time_step % 50 == 0:\n",
    "                    env.render(map_type=\"visit_count\")\n",
    "                    save_breakpoint()\n",
    "                    epsilon = epsilon * epsilon_decay if epsilon > epsilon_min else epsilon_min\n",
    "                    print(f'epsilon is {epsilon}')\n",
    "                    print(f\"Model and replay buffer are saved under {breakpoint_path} at time step: {time_step}\")\n",
    "\n",
    "            # Otherwise, keep adding data to replay buffer\n",
    "            else:\n",
    "                print(f\"Still adding data to replay buffer. Current replay buffer length: {len(replay_buffer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Let's test!\n",
    "1. **Environment Reset**:\n",
    "   - Resets the environment and prepares the initial state. The environment is the same as training environment.\n",
    "  \n",
    "2. **Loop Over Time Steps**:\n",
    "   - Loop through a maximum number of time steps (`time_steps`), representing agent actions within the environment.\n",
    "\n",
    "3. **Select and Execute an Action**:\n",
    "   - Selects an action based on the current policy (using an epsilon-greedy strategy) and takes a step in the environment to obtain the next state, reward, and done flag.\n",
    "\n",
    "4. **Render and Prepare Next State**:\n",
    "   - Renders the environment (to visualize changes) and processes the next state for input into the network.\n",
    "\n",
    "5. **Store Experience**:\n",
    "   - The current experience `(state, action, reward, next_state, done)` is added to the replay buffer to be used later for training.\n",
    "\n",
    "6. **Update Rewards**:\n",
    "   - Adds the reward received at the current time step to the total reward for the episode.\n",
    "\n",
    "7.  **Test and train the Model if Replay Buffer is Large Enough**:\n",
    "    - If the replay buffer contains enough samples, the DQN model is trained using a batch of experiences.\n",
    "\n",
    "8.  **Store Total Reward and Update Epsilon**:\n",
    "    - Appends the total reward for the episode to `rewards_list` and decays epsilon to gradually reduce exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------Time step: 0------------------------\n",
      "Ready to take random action:3\n",
      "Car moves Right 3 units from (25, 5) to (25, 8)\n",
      "Cells visited: 0/900 at time step: 0\n",
      "Collision rate: 0/0\n",
      "\n",
      "-------------------Time step: 1------------------------\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "Ready to take optimal action:2 with q_values:[[ -8.751141  -10.762853   -7.8785076  -8.763107 ]]\n",
      "Car moves Left 3 units from (25, 8) to (25, 5)\n",
      "Cells visited: 100/900 at time step: 1\n",
      "Collision rate: 0/1\n",
      "\n",
      "-------------------Time step: 2------------------------\n",
      "Ready to take random action:1\n",
      "Car moves Down 3 units from (25, 5) to (28, 5)\n",
      "Cells visited: 130/900 at time step: 2\n",
      "Collision rate: 0/2\n",
      "\n",
      "-------------------Time step: 3------------------------\n",
      "Ready to take random action:0\n",
      "Car moves Up 3 units from (28, 5) to (25, 5)\n",
      "Cells visited: 130/900 at time step: 3\n",
      "Collision rate: 0/3\n",
      "\n",
      "-------------------Time step: 4------------------------\n",
      "Ready to take random action:1\n",
      "Car moves Down 3 units from (25, 5) to (28, 5)\n",
      "Cells visited: 130/900 at time step: 4\n",
      "Collision rate: 0/4\n",
      "\n",
      "-------------------Time step: 5------------------------\n",
      "Ready to take random action:1\n",
      "Collision! Car stays in the same position:  (28, 5)\n",
      "Cells visited: 130/900 at time step: 5\n",
      "Collision rate: 1/5\n",
      "\n",
      "-------------------Time step: 6------------------------\n",
      "Ready to take random action:1\n",
      "Collision! Car stays in the same position:  (28, 5)\n",
      "Cells visited: 130/900 at time step: 6\n",
      "Collision rate: 2/6\n",
      "\n",
      "-------------------Time step: 7------------------------\n",
      "Ready to take random action:1\n",
      "Collision! Car stays in the same position:  (28, 5)\n",
      "Cells visited: 130/900 at time step: 7\n",
      "Collision rate: 3/7\n",
      "\n",
      "-------------------Time step: 8------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ready to take optimal action:2 with q_values:[[ -8.751141  -10.762853   -7.8785076  -8.763107 ]]\n",
      "Car moves Left 3 units from (28, 5) to (28, 2)\n",
      "Cells visited: 130/900 at time step: 8\n",
      "Collision rate: 3/8\n",
      "\n",
      "-------------------Time step: 9------------------------\n",
      "Ready to take random action:0\n",
      "Car moves Up 3 units from (28, 2) to (25, 2)\n",
      "Cells visited: 130/900 at time step: 9\n",
      "Collision rate: 3/9\n",
      "\n",
      "-------------------Time step: 10------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ready to take optimal action:2 with q_values:[[ -8.751141  -10.762853   -7.8785076  -8.763107 ]]\n",
      "Collision! Car stays in the same position:  (25, 2)\n",
      "Cells visited: 130/900 at time step: 10\n",
      "Collision rate: 4/10\n",
      "\n",
      "-------------------Time step: 11------------------------\n",
      "Ready to take random action:0\n",
      "Car moves Up 3 units from (25, 2) to (22, 2)\n",
      "Cells visited: 130/900 at time step: 11\n",
      "Collision rate: 4/11\n",
      "\n",
      "-------------------Time step: 12------------------------\n",
      "Ready to take random action:1\n",
      "Car moves Down 3 units from (22, 2) to (25, 2)\n",
      "Cells visited: 151/900 at time step: 12\n",
      "Collision rate: 4/12\n",
      "\n",
      "-------------------Time step: 13------------------------\n",
      "Ready to take random action:3\n",
      "Car moves Right 3 units from (25, 2) to (25, 5)\n",
      "Cells visited: 151/900 at time step: 13\n",
      "Collision rate: 4/13\n",
      "\n",
      "-------------------Time step: 14------------------------\n",
      "Ready to take random action:3\n",
      "Car moves Right 3 units from (25, 5) to (25, 8)\n",
      "Cells visited: 151/900 at time step: 14\n",
      "Collision rate: 4/14\n",
      "\n",
      "-------------------Time step: 15------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Ready to take optimal action:2 with q_values:[[ -8.751141  -10.762853   -7.8785076  -8.763107 ]]\n",
      "Car moves Left 3 units from (25, 8) to (25, 5)\n",
      "Cells visited: 151/900 at time step: 15\n",
      "Collision rate: 4/15\n",
      "\n",
      "-------------------Time step: 16------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Ready to take optimal action:2 with q_values:[[ -8.751141  -10.762853   -7.8785076  -8.763107 ]]\n",
      "Car moves Left 3 units from (25, 5) to (25, 2)\n",
      "Cells visited: 151/900 at time step: 16\n",
      "Collision rate: 4/16\n",
      "\n",
      "-------------------Time step: 17------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Ready to take optimal action:2 with q_values:[[ -8.751141  -10.762853   -7.8785076  -8.763107 ]]\n",
      "Collision! Car stays in the same position:  (25, 2)\n",
      "Cells visited: 151/900 at time step: 17\n",
      "Collision rate: 5/17\n",
      "\n",
      "-------------------Time step: 18------------------------\n",
      "Ready to take random action:3\n",
      "Car moves Right 3 units from (25, 2) to (25, 5)\n",
      "Cells visited: 151/900 at time step: 18\n",
      "Collision rate: 5/18\n",
      "\n",
      "-------------------Time step: 19------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ready to take optimal action:2 with q_values:[[ -8.751141  -10.762853   -7.8785076  -8.763107 ]]\n",
      "Car moves Left 3 units from (25, 5) to (25, 2)\n",
      "Cells visited: 151/900 at time step: 19\n",
      "Collision rate: 5/19\n",
      "\n",
      "-------------------Time step: 20------------------------\n",
      "Ready to take random action:0\n",
      "Car moves Up 3 units from (25, 2) to (22, 2)\n",
      "Cells visited: 151/900 at time step: 20\n",
      "Collision rate: 5/20\n",
      "\n",
      "-------------------Time step: 21------------------------\n",
      "Ready to take random action:3\n",
      "Car moves Right 3 units from (22, 2) to (22, 5)\n",
      "Cells visited: 151/900 at time step: 21\n",
      "Collision rate: 5/21\n",
      "\n",
      "-------------------Time step: 22------------------------\n",
      "Ready to take random action:0\n",
      "Car moves Up 3 units from (22, 5) to (19, 5)\n",
      "Cells visited: 160/900 at time step: 22\n",
      "Collision rate: 5/22\n",
      "\n",
      "-------------------Time step: 23------------------------\n",
      "Ready to take random action:2\n",
      "Car moves Left 3 units from (19, 5) to (19, 2)\n",
      "Cells visited: 190/900 at time step: 23\n",
      "Collision rate: 5/23\n",
      "\n",
      "-------------------Time step: 24------------------------\n",
      "Ready to take random action:2\n",
      "Collision! Car stays in the same position:  (19, 2)\n",
      "Cells visited: 190/900 at time step: 24\n",
      "Collision rate: 6/24\n",
      "\n",
      "-------------------Time step: 25------------------------\n",
      "Ready to take random action:2\n",
      "Collision! Car stays in the same position:  (19, 2)\n",
      "Cells visited: 190/900 at time step: 25\n",
      "Collision rate: 7/25\n",
      "\n",
      "-------------------Time step: 26------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ready to take optimal action:2 with q_values:[[ -8.751141  -10.762853   -7.8785076  -8.763107 ]]\n",
      "Collision! Car stays in the same position:  (19, 2)\n",
      "Cells visited: 190/900 at time step: 26\n",
      "Collision rate: 8/26\n",
      "\n",
      "-------------------Time step: 27------------------------\n",
      "Ready to take random action:0\n",
      "Car moves Up 3 units from (19, 2) to (16, 2)\n",
      "Cells visited: 190/900 at time step: 27\n",
      "Collision rate: 8/27\n",
      "\n",
      "-------------------Time step: 28------------------------\n",
      "Ready to take random action:0\n",
      "Car moves Up 3 units from (16, 2) to (13, 2)\n",
      "Cells visited: 211/900 at time step: 28\n",
      "Collision rate: 8/28\n",
      "\n",
      "-------------------Time step: 29------------------------\n",
      "Ready to take random action:1\n",
      "Car moves Down 3 units from (13, 2) to (16, 2)\n",
      "Cells visited: 232/900 at time step: 29\n",
      "Collision rate: 8/29\n",
      "\n",
      "-------------------Time step: 30------------------------\n",
      "Ready to take random action:0\n",
      "Car moves Up 3 units from (16, 2) to (13, 2)\n",
      "Cells visited: 232/900 at time step: 30\n",
      "Collision rate: 8/30\n",
      "\n",
      "-------------------Time step: 31------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Ready to take optimal action:2 with q_values:[[ -8.751141  -10.762853   -7.8785076  -8.763107 ]]\n",
      "Collision! Car stays in the same position:  (13, 2)\n",
      "Cells visited: 232/900 at time step: 31\n",
      "Collision rate: 9/31\n",
      "\n",
      "-------------------Time step: 32------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ready to take optimal action:2 with q_values:[[ -8.751141  -10.762853   -7.8785076  -8.763107 ]]\n",
      "Collision! Car stays in the same position:  (13, 2)\n",
      "Cells visited: 232/900 at time step: 32\n",
      "Collision rate: 10/32\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 33------------------------\n",
      "Ready to take random action:3\n",
      "Collision! Car stays in the same position:  (13, 2)\n",
      "Cells visited: 232/900 at time step: 33\n",
      "Collision rate: 11/33\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 34------------------------\n",
      "Ready to take random action:3\n",
      "Collision! Car stays in the same position:  (13, 2)\n",
      "Cells visited: 232/900 at time step: 34\n",
      "Collision rate: 12/34\n",
      "Start training DQN model with batch size: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------Time step: 35------------------------\n",
      "Ready to take random action:3\n",
      "Collision! Car stays in the same position:  (13, 2)\n",
      "Cells visited: 232/900 at time step: 35\n",
      "Collision rate: 13/35\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 36------------------------\n",
      "Ready to take random action:3\n",
      "Collision! Car stays in the same position:  (13, 2)\n",
      "Cells visited: 232/900 at time step: 36\n",
      "Collision rate: 14/36\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 37------------------------\n",
      "Ready to take random action:1\n",
      "Car moves Down 3 units from (13, 2) to (16, 2)\n",
      "Cells visited: 232/900 at time step: 37\n",
      "Collision rate: 14/37\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 38------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ready to take optimal action:0 with q_values:[[-6.438986  -9.343675  -8.609418  -7.0497913]]\n",
      "Car moves Up 3 units from (16, 2) to (13, 2)\n",
      "Cells visited: 232/900 at time step: 38\n",
      "Collision rate: 14/38\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 39------------------------\n",
      "Ready to take random action:0\n",
      "Car moves Up 3 units from (13, 2) to (10, 2)\n",
      "Cells visited: 232/900 at time step: 39\n",
      "Collision rate: 14/39\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 40------------------------\n",
      "Ready to take random action:2\n",
      "Collision! Car stays in the same position:  (10, 2)\n",
      "Cells visited: 253/900 at time step: 40\n",
      "Collision rate: 15/40\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 41------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ready to take optimal action:0 with q_values:[[-5.0380335 -8.644816  -7.778417  -7.4858804]]\n",
      "Car moves Up 3 units from (10, 2) to (7, 2)\n",
      "Cells visited: 253/900 at time step: 41\n",
      "Collision rate: 15/41\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 42------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ready to take optimal action:0 with q_values:[[-3.8097742 -8.616563  -7.591924  -6.832383 ]]\n",
      "Car moves Up 3 units from (7, 2) to (4, 2)\n",
      "Cells visited: 274/900 at time step: 42\n",
      "Collision rate: 15/42\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 43------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ready to take optimal action:0 with q_values:[[-3.4623485 -8.132699  -6.646261  -6.831908 ]]\n",
      "Car moves Up 3 units from (4, 2) to (1, 2)\n",
      "Cells visited: 288/900 at time step: 43\n",
      "Collision rate: 15/43\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 44------------------------\n",
      "Ready to take random action:1\n",
      "Car moves Down 3 units from (1, 2) to (4, 2)\n",
      "Cells visited: 288/900 at time step: 44\n",
      "Collision rate: 15/44\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 45------------------------\n",
      "Ready to take random action:2\n",
      "Collision! Car stays in the same position:  (4, 2)\n",
      "Cells visited: 288/900 at time step: 45\n",
      "Collision rate: 16/45\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 46------------------------\n",
      "Ready to take random action:2\n",
      "Collision! Car stays in the same position:  (4, 2)\n",
      "Cells visited: 288/900 at time step: 46\n",
      "Collision rate: 17/46\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 47------------------------\n",
      "Ready to take random action:1\n",
      "Car moves Down 3 units from (4, 2) to (7, 2)\n",
      "Cells visited: 288/900 at time step: 47\n",
      "Collision rate: 17/47\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 48------------------------\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ready to take optimal action:0 with q_values:[[-1.1378949 -5.2969093 -4.6128817 -2.311    ]]\n",
      "Car moves Up 3 units from (7, 2) to (4, 2)\n",
      "Cells visited: 288/900 at time step: 48\n",
      "Collision rate: 17/48\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 49------------------------\n",
      "Ready to take random action:3\n",
      "Collision! Car stays in the same position:  (4, 2)\n",
      "Cells visited: 288/900 at time step: 49\n",
      "Collision rate: 18/49\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 50------------------------\n",
      "Ready to take random action:1\n",
      "Car moves Down 3 units from (4, 2) to (7, 2)\n",
      "Cells visited: 288/900 at time step: 50\n",
      "Collision rate: 18/50\n",
      "Start training DQN model with batch size: 32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------Time step: 51------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ready to take optimal action:0 with q_values:[[-0.5935428 -3.201303  -3.581358  -1.7767167]]\n",
      "Car moves Up 3 units from (7, 2) to (4, 2)\n",
      "Cells visited: 288/900 at time step: 51\n",
      "Collision rate: 18/51\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 52------------------------\n",
      "Ready to take random action:0\n",
      "Car moves Up 3 units from (4, 2) to (1, 2)\n",
      "Cells visited: 288/900 at time step: 52\n",
      "Collision rate: 18/52\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 53------------------------\n",
      "Ready to take random action:3\n",
      "Car moves Right 3 units from (1, 2) to (1, 5)\n",
      "Cells visited: 288/900 at time step: 53\n",
      "Collision rate: 18/53\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 54------------------------\n",
      "Ready to take random action:3\n",
      "Car moves Right 3 units from (1, 5) to (1, 8)\n",
      "Cells visited: 306/900 at time step: 54\n",
      "Collision rate: 18/54\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 55------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ready to take optimal action:0 with q_values:[[ 0.46249616 -1.9191108  -3.0943873  -1.9774681 ]]\n",
      "Collision! Car stays in the same position:  (1, 8)\n",
      "Cells visited: 324/900 at time step: 55\n",
      "Collision rate: 19/55\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 56------------------------\n",
      "Ready to take random action:0\n",
      "Collision! Car stays in the same position:  (1, 8)\n",
      "Cells visited: 324/900 at time step: 56\n",
      "Collision rate: 20/56\n",
      "Start training DQN model with batch size: 32\n",
      "\n",
      "-------------------Time step: 57------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Ready to take optimal action:0 with q_values:[[ 1.5617146  -1.8500462  -2.2224452  -0.27580547]]\n",
      "Collision! Car stays in the same position:  (1, 8)\n",
      "Cells visited: 324/900 at time step: 57\n",
      "Collision rate: 21/57\n",
      "Start training DQN model with batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "if train_mode == 'test':\n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    state,_ = prepare_state(state)\n",
    "    replay_buffer = deque(maxlen=2000)\n",
    "    images = []\n",
    "    titles = []\n",
    "    collision_time = 0\n",
    "\n",
    "    # Loop through each time step\n",
    "    for time_step in range(time_steps): \n",
    "        print(f\"\\n-------------------Time step: {time_step}------------------------\")\n",
    "\n",
    "        # Prepare the images and titles for the gif\n",
    "        combined_image = combine_all(env)\n",
    "        images.append(combined_image / np.max(combined_image) * 255)\n",
    "        titles.append(f\"{time_step}\")\n",
    "\n",
    "        # In each time step, agent selects an action and implement it, environment provides feedback\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            env.render(map_type=\"visit_count\")\n",
    "            save_to_gif(images,'test_dqn_gif', f'policy_by_DQN_fully_explored{time.strftime(\"%m-%d_%H-%M\")}.gif', titles)\n",
    "            print(\"ALL DONE\")\n",
    "            break\n",
    "\n",
    "        # Calculate the number of cells visited\n",
    "        cells_visited = state[0][state[0] == EXPLORED].shape[0]\n",
    "        cells = env.map_size[0]*env.map_size[1]\n",
    "        print(f\"Cells visited: {cells_visited}/{env.map_size[0]*env.map_size[1]} at time step: {time_step}\")\n",
    "\n",
    "        # Collision rate\n",
    "        collision_time = collision_time + 1 if env.COLLISION_FLAG else collision_time\n",
    "        print(f\"Collision rate: {collision_time}/{time_step}\")\n",
    "\n",
    "        # Store the data into replay buffer\n",
    "        next_state,_ = prepare_state(next_state)\n",
    "        replay_buffer.append((state, action, reward, next_state, done)) # 将数据存入replay buffer\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Training while testing\n",
    "        if len(replay_buffer) > batch_size: # 当步数大于batch size时开始训练\n",
    "            print(f\"Start training DQN model with batch size: {batch_size}\")\n",
    "            losses = train_dqn(batch_size)\n",
    "\n",
    "            # if time_step % 3 == 0:\n",
    "            #     save_to_gif(images,'test_dqn_gif', f'policy_by_DQN_{time.strftime(\"%m-%d_%H-%M\")}.gif', titles)\n",
    "\n",
    "            # If we are in train mode, save the gym, model and replay buffer for emergency stop every 100 time steps\n",
    "            if time_step % 50 == 0:\n",
    "                save_to_gif(images,'test_dqn_gif', f'policy_by_DQN_{time.strftime(\"%m-%d_%H-%M\")}.gif', titles)\n",
    "                env.render(map_type=\"visit_count\")\n",
    "                epsilon = epsilon * epsilon_decay if epsilon > epsilon_min else epsilon_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
