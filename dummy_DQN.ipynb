{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import modules and run an example of dummy_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from dummy_gym import DummyGym,EXPLORED\n",
    "import pickle\n",
    "import imageio\n",
    "\n",
    "# if there is a GPU available, set train the device to GPU\n",
    "device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Only I need to do this because I am running the code on my local machine\n",
    "# os.chdir('/home/zhihan/ME5418')\n",
    "\n",
    "# Create the environment instance\n",
    "env = DummyGym()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters setting\n",
    "In order to test our model, we did not assign large parameters. Set the training mode using one of the following options:\n",
    "### Training Mode Options\n",
    "- **`train`**: Start training from scratch, overwriting existing model, state, and replay buffer files.\n",
    "- **`test`**: Test the model without overwriting any files.\n",
    "- **`train_from_breakpoint`**: Continue training from the last saved breakpoint.\n",
    "- **`fine_tune`**: Fine-tune the model while preserving the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 断点文件会保存在train_breakpoint文件夹中\n",
    "# 如果中途训练中断，可以从断点处继续训练，只需将train_mode设置为'train_from_breakpoint'，包括模型文件、状态文件和replay_buffer文件\n",
    "# 如果想重新训练，将train_mode设置为'train'即可，模型文件、状态文件和replay_buffer文件会被覆盖\n",
    "# 如果想测试模型，将train_mode设置为'test'即可，模型文件、状态文件和replay_buffer文件不会被覆盖\n",
    "# 如果既想重新训练，又想保留之前的模型，即微调模型，将train_mode设置为'fine_tune'即可\n",
    "\n",
    "train_mode = 'fine_tune'\n",
    "# train_mode = 'train' # 'train' or 'test' or 'train_from_breakpoint' or 'fine_tune'\n",
    "model_name = 'dqn_model.h5'\n",
    "state_name = 'state.npz'\n",
    "replay_buffer_name = 'replay_buffer.pkl'\n",
    "\n",
    "breakpoint_path = 'train_breakpoint'\n",
    "model_path = os.path.join(breakpoint_path, model_name)\n",
    "state_path = os.path.join(breakpoint_path, state_name)\n",
    "replay_buffer_path = os.path.join(breakpoint_path, replay_buffer_name)\n",
    "\n",
    "# Hyperparameters\n",
    "original_state = env.observe()\n",
    "action_size = env.action_space.n\n",
    "print(f'action_size is {action_size}')\n",
    "batch_size = 32 # 每次训练的数据量\n",
    "n_episodes = 50 # 地图数量\n",
    "time_steps = 300 # 每张地图最多走多少步，避免陷入局部重复导致训练无效数据\n",
    "epochs = 10 # 每个地图训练次数\n",
    "\n",
    "# DQN parameters\n",
    "gamma = 0.99  # discount rate, 0.99 means the future rewards are considered important\n",
    "epsilon = 0.5 # 探索率\n",
    "epsilon_min = 0.01 # 最小探索率\n",
    "epsilon_decay = 0.995 # 探索衰减率\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer = deque(maxlen=2000) # 用于存储训练数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing states\n",
    "The state we get from dummy_gym are lists and have only 2 dimensions. So we have to\n",
    "1. Use `np.array` to change the type of state to `ndarray` for future data processing\n",
    "2. Use `map.reshape((1, *map.shape, 1))` to reshape the state to 4 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理原始state\n",
    "def prepare_state(original_state):\n",
    "    '''\n",
    "    Prepare the state for the model by numpying and adding dimension\n",
    "    '''\n",
    "    new_state = []\n",
    "    new_state_shape = []\n",
    "    for state in original_state:\n",
    "        state = np.array(state)\n",
    "        # add dimension\n",
    "        state = state.reshape(1, *state.shape, 1) # (30, 30) -> (1, 30, 30, 1)\n",
    "        new_state.append(state)\n",
    "        new_state_shape.append(state.shape)\n",
    "\n",
    "    return new_state, new_state_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, state_shape = prepare_state(original_state)\n",
    "state_shape[0][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Designing out model\n",
    "After comparing different model output, we found that model behaves best when follows these settings:\n",
    "1. `pool=True`\n",
    "2. `dropout=True`\n",
    "3. `residual=True`\n",
    "4. `batch_norm=True`\n",
    "5. `regularizer=None`\n",
    "\n",
    "You can see the summay of the model in below ceil.\\\n",
    "**Model is loaded from `model_path` instead of being created if `train_mode` is set `fine_tune` or `train_from_breakpoint`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_model(conv_filters_1=32, conv_filters_2=16, kernel_size=(3,3), activation='sigmoid', \n",
    "                     dense_units_1=64, dense_units_2=32, pool_size=(2,2), padding='same',\n",
    "                     pool=True, dropout=True, residual=True, batch_norm=True, regularizer=None, LSTM=False, flat=True, dense_twice=True): # 多一个全连接多2000参数\n",
    "                                                                                                                                           # 如果flat=False参数直接减少183万，但会丢失空间信息，因为GlobalAveragePooling2D会平均所有值\n",
    "    # Input for state_array[0].shape array(120*120)\n",
    "    input_visit_count_state = tf.keras.layers.Input(shape=state_shape[0][1:], name='input_visit_count_state')\n",
    "    conv_visit_count_state = tf.keras.layers.Conv2D(conv_filters_1, kernel_size, activation=activation, padding=padding)(input_visit_count_state)\n",
    "    conv_visit_count_state = tf.keras.layers.Conv2D(conv_filters_1, kernel_size, activation=activation, padding=padding)(conv_visit_count_state)\n",
    "    # print(conv_visit_count_state.shape)\n",
    "\n",
    "    # different settings    \n",
    "    batch_norm_visit_count_state = tf.keras.layers.BatchNormalization()(conv_visit_count_state) if batch_norm else conv_visit_count_state\n",
    "    residual_visit_count_state = tf.keras.layers.Add()([input_visit_count_state, batch_norm_visit_count_state])     if residual else batch_norm_visit_count_state\n",
    "    dropout_visit_count_state  = tf.keras.layers.Dropout(0.2)(residual_visit_count_state)                     if dropout  else residual_visit_count_state\n",
    "    pool_visit_count_state     = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(dropout_visit_count_state) if pool     else dropout_visit_count_state\n",
    "    \n",
    "    flat_visit_count_state = tf.keras.layers.Flatten()(pool_visit_count_state) if flat == True else tf.keras.layers.GlobalAveragePooling2D()(pool_visit_count_state)\n",
    "\n",
    "    # Input for state_array[1].shape 10x10 array\n",
    "    input_fov_map = tf.keras.layers.Input(shape=state_shape[1][1:], name='input_fov_map')\n",
    "    conv_fov_map = tf.keras.layers.Conv2D(conv_filters_2, kernel_size, activation=activation, padding=padding)(input_fov_map)\n",
    "    conv_fov_map = tf.keras.layers.Conv2D(conv_filters_2, kernel_size, activation=activation, padding=padding)(conv_fov_map)\n",
    "    # print(conv_fov_map.shape)\n",
    "\n",
    "    # different settings\n",
    "    batch_norm_fov_map = tf.keras.layers.BatchNormalization()(conv_fov_map) if batch_norm else conv_fov_map\n",
    "    residual_fov_map = tf.keras.layers.Add()([input_fov_map, batch_norm_fov_map]) if residual else batch_norm_fov_map\n",
    "    dropout_fov_map_state = tf.keras.layers.Dropout(0.2)(residual_fov_map) if dropout else residual_fov_map\n",
    "    pool_fov_map_state = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(dropout_fov_map_state) if pool else dropout_fov_map_state\n",
    "    \n",
    "    flat_fov_map = tf.keras.layers.Flatten()(pool_fov_map_state) if flat == True else tf.keras.layers.GlobalAveragePooling2D()(pool_fov_map_state)\n",
    "\n",
    "    # Input for 2x1 array\n",
    "    input_car_pos = tf.keras.layers.Input(shape=state_shape[2][1:], name='input_car_pos')\n",
    "    dense_car_pos = tf.keras.layers.Flatten()(input_car_pos)\n",
    "\n",
    "    # Concatenate all branches\n",
    "    combined = tf.keras.layers.Concatenate()([flat_visit_count_state, flat_fov_map, dense_car_pos])\n",
    "\n",
    "    # Add LSTM layer for sequential processing\n",
    "    lstm_layer = tf.keras.layers.Reshape((1, combined.shape[1]))(combined)  # Reshape for LSTM input\n",
    "    lstm_layer = tf.keras.layers.LSTM(64, activation=activation)(lstm_layer)  # LSTM layer added\n",
    "\n",
    "    # Fully connected layers after concatenation\n",
    "    dense1 = tf.keras.layers.Dense(dense_units_1, activation=activation, kernel_regularizer=regularizer)(lstm_layer if LSTM else combined)\n",
    "    dense2 = tf.keras.layers.Dense(dense_units_2, activation=activation, kernel_regularizer=regularizer)(dense1) if dense_twice else dense1\n",
    "    output = tf.keras.layers.Dense(action_size, activation='linear', kernel_regularizer=regularizer)(dense2)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[input_visit_count_state, input_fov_map, input_car_pos], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the DQN model and target model\n",
    "if train_mode == 'train_from_breakpoint' or train_mode == 'fine_tune' or train_mode == 'test':\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    target_model = tf.keras.models.load_model(model_path)\n",
    "elif train_mode == 'train':\n",
    "    model = create_dqn_model()\n",
    "    target_model = create_dqn_model()\n",
    "    target_model.set_weights(model.get_weights())\n",
    "else:\n",
    "    raise ValueError('train_mode should be either train, test, fine_tune or train_from_breakpoint')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select actions\n",
    "Actions are selected randomly with the possibility of epsilon. Otherwise will be selected based on `q_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon, CHANGE_ACTION_FLAG=0):\n",
    "    '''\n",
    "    Select action using epsilon-greedy policy\n",
    "    '''\n",
    "    if train_mode != 'test':\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = random.randrange(action_size)  # Explore: random action\n",
    "            print(f'Ready to take random action:{action}')\n",
    "        else:\n",
    "            env.render(map_type=\"visit_count\")\n",
    "            q_values = model.predict(state)  # Exploit: select action with max Q-value\n",
    "            action_list = np.argsort(q_values[0])[::-1]\n",
    "            action = action_list[0]\n",
    "            print(f'Ready to take optimal action:{action} with q_values:{q_values}')\n",
    "    else:\n",
    "        env.render(map_type=\"visit_count\")\n",
    "        q_values = model.predict(state)  # Exploit: select action with max Q-value\n",
    "        action_list = np.argsort(q_values[0])[::-1]\n",
    "        action = action_list[CHANGE_ACTION_FLAG]\n",
    "        print(f'Ready to take action:{action} with q_values:{q_values} after {CHANGE_ACTION_FLAG} changes')\n",
    "    \n",
    "    return action  # Exploit: select action with max Q-value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define function of trainnign model\n",
    "1. **Sample a Minibatch**:\n",
    "   - A minibatch of experiences is randomly sampled from the `replay_buffer` to train the model. Each experience is in the format `(state, action, reward, next_state, done)`.\n",
    "\n",
    "2. **Loop through each Experience**:\n",
    "   - The code iterates through each experience in the minibatch to update the model. \n",
    "\n",
    "3. **Initialize Target**:\n",
    "   - The immediate reward is set as the initial target.\n",
    "\n",
    "4. **Adjust Batch Dimension**:\n",
    "   - This step ensures that the state tensors have a batch dimension (shape of 4D) by expanding dimensions if needed.\n",
    "\n",
    "5. **Compute Target if Episode Not Done**:\n",
    "   - If the episode is not finished (`done` is `False`), the target Q-value is updated using the formula:\\\n",
    "     $\n",
    "     \\text{target} = \\text{reward} + \\gamma \\cdot \\max_{a'} Q_{\\text{target_model}}(\\text{next_state}, a')\n",
    "     $\n",
    "   - `gamma` is the discount factor that weighs the contribution of future rewards. The target model is used to predict the Q-values of the next state.\n",
    "\n",
    "6. **Update Predicted Q-Values**:\n",
    "   - The current state's Q-values are predicted, and the Q-value for the action taken is updated with the calculated target value.\n",
    "\n",
    "7. **Train the Model**:\n",
    "   - The model is trained on the current state and updated Q-values for `epochs` iterations. The loss for each training step is stored in `history`.\n",
    "\n",
    "8. **Plot Loss**:\n",
    "   - This section plots the loss curve for the training steps within this minibatch.\n",
    "\n",
    "9. **Update Target Model Weights**:\n",
    "   - After training, the weights of the target model are updated to match the main model. This helps stabilize training by having a slightly outdated model (target model) to compare predictions.\n",
    "\n",
    "10. **Return the Losses**:\n",
    "    - The function returns the loss values for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(batch_size): # learning agent\n",
    "    with tf.device(device):\n",
    "        minibatch = random.sample(replay_buffer, batch_size) # Sample minibatch from the replay buffer\n",
    "        for state, action, reward, next_state, done in minibatch: # 从小批量样本中获取数据,32*avg_loss_decrease=64\n",
    "            target = reward + gamma * np.amax(target_model.predict(next_state,verbose=0)[0])# 当前状态的reward作为target\n",
    "            target_q_values = target_model.predict(state,verbose=0) # 得到4个动作的Q值\n",
    "            target_q_values[0][action] = target # 将对应动作的Q值更新为target\n",
    "            \n",
    "            history = model.fit(state, target_q_values, epochs=epochs, verbose=0) # 对这个小批量样本训练epochs次,verbose=1表示显示训练进度条\n",
    "\n",
    "        # Update target model weights\n",
    "        target_model.set_weights(model.get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start tranning!\n",
    "1. **Define saving and loading breakpoint function**:\n",
    "   - In case of emergency like sudden death of kernel, function of saving and recovery should be defined.\n",
    "\n",
    "2. **Loop Over Episodes**:\n",
    "   - Iterates through a set number of episodes (`n_episodes`), each representing a complete run of the environment.\n",
    "\n",
    "3. **Environment Reset**:\n",
    "   - Resets the environment at the start of each episode and prepares the initial state.\n",
    "\n",
    "4. **Loop Over Time Steps**:\n",
    "   - Within each episode, it loops through a maximum number of time steps (`time_steps`), representing agent actions within the environment.\n",
    "\n",
    "5. **Select and Execute an Action**:\n",
    "   - Selects an action based on the current policy (using an epsilon-greedy strategy) and takes a step in the environment to obtain the next state, reward, and done flag.\n",
    "\n",
    "6. **Render and Prepare Next State**:\n",
    "   - Renders the environment (to visualize changes) and processes the next state for input into the network.\n",
    "\n",
    "7. **Store Experience**:\n",
    "   - The current experience `(state, action, reward, next_state, done)` is added to the replay buffer to be used later for training.\n",
    "\n",
    "8. **Update Rewards**:\n",
    "   - Adds the reward received at the current time step to the total reward for the episode.\n",
    "\n",
    "9. **Check if Episode is Complete**:\n",
    "   - Ends the episode if the `done` flag is set to `True` and prints the episode's result.\n",
    "\n",
    "10. **Train the Model if Replay Buffer is Large Enough**:\n",
    "    - If the replay buffer contains enough samples, the DQN model is trained using a batch of experiences.\n",
    "\n",
    "11. **Store Total Reward and Update Epsilon**:\n",
    "    - Appends the total reward for the episode to `rewards_list` and decays epsilon to gradually reduce exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def save_breakpoint():\n",
    "    model.save(model_path)\n",
    "    env.save_state(state_path)\n",
    "    with open(replay_buffer_path, 'wb') as f:\n",
    "        pickle.dump(replay_buffer, f)\n",
    "\n",
    "def load_breakpoint():\n",
    "    # Model already loaded\n",
    "    with open(replay_buffer_path, 'rb') as f:\n",
    "        replay_buffer = pickle.load(f)\n",
    "    env.load_state(state_path)\n",
    "    return replay_buffer\n",
    "\n",
    "def save_to_gif(image_list, output_path):\n",
    "    # 将二维数组转换为图像对象\n",
    "    print(f'Saving GIF with image list length of {len(image_list)}')\n",
    "    imageio.mimsave(output_path, image_list, duration=0.5)\n",
    "\n",
    "    print(f\"GIF 保存成功：{output_path}\")\n",
    "\n",
    "if train_mode=='train_from_breakpoint':\n",
    "    replay_buffer = load_breakpoint()\n",
    "    print('Loading model and replay buffer from breakpoint')\n",
    "    env.render(map_type='visit_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if train_mode != 'test':\n",
    "    for episode in range(n_episodes): # 训练多少张图\n",
    "        print(f\"\\n********************Episode: {episode+1}/{n_episodes}********************\")\n",
    "\n",
    "        # Reset the environment and get the initial state\n",
    "        state = env.state if train_mode== 'train_from_breakpoint' else env.reset()\n",
    "        state,_ = prepare_state(state)\n",
    "        epsilon = epsilon * epsilon_decay if epsilon > epsilon_min else epsilon_min\n",
    "        replay_buffer = deque(maxlen=2000)\n",
    "        images = []\n",
    "\n",
    "        # Loop through each time step\n",
    "        for time_step in range(time_steps): \n",
    "            print(f\"\\n-------------------Time step: {time_step}------------------------\")\n",
    "            images.append(env.visit_count)\n",
    "            # In each time step, agent selects an action and implement it, environment provides feedback\n",
    "            action = select_action(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    #         print(f\"Reward after this action:{reward}\")\n",
    "            if done or time_step>300:\n",
    "                env.render(map_type=\"visit_count\")\n",
    "                print(\"This eqisode is done, ready for next one.\")\n",
    "                break\n",
    "            cells_visited = state[0][state[0] == EXPLORED].shape[0] # 计算已经访问的cell数量\n",
    "            cells = env.map_size[0]*env.map_size[1]\n",
    "            print(f\"Cells visited: {cells_visited}/{env.map_size[0]*env.map_size[1]} at time step: {time_step}\")\n",
    "\n",
    "            # Store the data into replay buffer\n",
    "            next_state,_ = prepare_state(next_state)\n",
    "            replay_buffer.append((state, action, reward, next_state, done)) # 将数据存入replay buffer\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            save_to_gif(images,'policy_by_DQN.gif') if time_step % 3 == 0 else None\n",
    "\n",
    "            # Train the model when replay buffer is larger than batch size\n",
    "            if len(replay_buffer) > batch_size: # 当步数大于batch size时开始训练\n",
    "                print(f\"Start training DQN model with batch size: {batch_size}\")\n",
    "                losses = train_dqn(batch_size)\n",
    "\n",
    "                # If we are in train mode, save the gym, model and replay buffer for emergency stop every 100 time steps\n",
    "                if (train_mode == 'train' or train_mode == 'fine_tune') and time_step % 50 == 0:\n",
    "                    env.render(map_type=\"visit_count\")\n",
    "                    save_breakpoint()\n",
    "                    print(f\"Model and replay buffer are saved under {breakpoint_path} at time step: {time_step}\")\n",
    "\n",
    "            # Otherwise, keep adding data to replay buffer\n",
    "            else:\n",
    "                print(f\"Still adding data to replay buffer. Current replay buffer length: {len(replay_buffer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the dqn_model.h5\n",
    "if train_mode == 'test':\n",
    "    tested_model = tf.keras.models.load_model(model_path)\n",
    "    state = env.reset()\n",
    "    state,_ = prepare_state(state)\n",
    "    time_step = 0\n",
    "    CHANGE_ACTION_FLAG = 0\n",
    "    while True:\n",
    "        print(f\"\\n-------------------Time step: {time_step}------------------------\")\n",
    "        print(tested_model.predict(state)[0])\n",
    "        action = select_action(state, epsilon, CHANGE_ACTION_FLAG)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            env.render(map_type=\"visit_count\")\n",
    "            print(\"This eqisode is done, ready for next one.\")\n",
    "            break\n",
    "        if time_step % 100 == 0:\n",
    "            env.render()\n",
    "        next_state,_ = prepare_state(next_state)\n",
    "        CHANGE_ACTION_FLAG = CHANGE_ACTION_FLAG + 1 if env.COLLISION_FLAG else 0\n",
    "        state = next_state\n",
    "        time_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
