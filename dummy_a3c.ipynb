{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "from dummy_gym import DummyGym\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = DummyGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "learning_rate = 0.0003\n",
    "gamma = 0.95\n",
    "num_episodes = 200\n",
    "global_update_freq = 5  # 每隔 5 个步骤更新一次全局模型\n",
    "\n",
    "\n",
    "\n",
    "# state return self.state（self.visit_count, self.fov_map, self.car.pos）, reward, done, {}\n",
    "# step()执行完之后自动返回。 observe()返回的self.visit_count, self.fov_map, self.car.pos 直接返回state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalModel(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(GlobalModel, self).__init__()\n",
    "        # 修改网络结构以更好地处理地图特征\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3, stride=1, padding=1)  # 输入通道改为2，包含当前状态和探索历史\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64 * 8 * 8, 512)  # 增加网络容量\n",
    "\n",
    "        self.policy_logits = nn.Linear(512, action_size)\n",
    "        self.value = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return self.policy_logits(x), self.value(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(mp.Process):\n",
    "    def __init__(self, global_model, optimizer, action_size, worker_id, gamma=0.95):\n",
    "        # ... existing code ...\n",
    "        self.exploration_history = np.zeros((8, 8))  # 添加探索历史记录\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        # 添加状态预处理\n",
    "        current_map = state[1]\n",
    "        # 将当前状态和探索历史组合\n",
    "        state_tensor = torch.tensor(\n",
    "            np.stack([current_map, self.exploration_history]), \n",
    "            dtype=torch.float32\n",
    "        ).unsqueeze(0)\n",
    "        return state_tensor\n",
    "\n",
    "    def run(self):\n",
    "        global_episode = 0\n",
    "        while global_episode < num_episodes:\n",
    "            state = self.env.reset()\n",
    "            current_state = self.preprocess_state(state)\n",
    "            self.exploration_history = np.zeros((8, 8))  # 重置探索历史\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.choose_action(current_state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # 更新探索历史\n",
    "                pos = next_state[0]  # 假设返回的状态包含智能体位置\n",
    "                self.exploration_history[int(pos[0]), int(pos[1])] = 1\n",
    "                \n",
    "                # 计算探索奖励\n",
    "                exploration_reward = 0.5 if self.exploration_history[int(pos[0]), int(pos[1])] == 0 else -0.1\n",
    "                total_reward = reward + exploration_reward\n",
    "                \n",
    "                next_state_tensor = self.preprocess_state(next_state)\n",
    "                loss = self.compute_loss(done, current_state, action, total_reward, next_state_tensor)\n",
    "                self.update_global(loss)\n",
    "                \n",
    "                episode_reward += total_reward\n",
    "                current_state = next_state_tensor\n",
    "\n",
    "                if done:\n",
    "                    coverage = np.sum(self.exploration_history) / (8 * 8)\n",
    "                    print(f\"Worker: {self.worker_id}, Episode: {global_episode}, \"\n",
    "                          f\"Reward: {episode_reward:.2f}, Coverage: {coverage:.2%}\")\n",
    "                    global_episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义全局模型\n",
    "class GlobalModel(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(GlobalModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        self.fc = nn.Linear(64 * 6 * 6, 256)  # 假设输入是一个 8x8 的网格\n",
    "\n",
    "        # 策略网络输出每个动作的概率\n",
    "        self.policy_logits = nn.Linear(256, action_size)\n",
    "        # 价值网络输出当前状态的价值\n",
    "        self.value = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc(x))\n",
    "        return self.policy_logits(x), self.value(x)\n",
    "\n",
    "# 定义 Worker 类\n",
    "class Worker(mp.Process):\n",
    "    def __init__(self, global_model, optimizer, action_size, worker_id, gamma=0.99):\n",
    "        super(Worker, self).__init__()\n",
    "        self.global_model = global_model\n",
    "        self.optimizer = optimizer\n",
    "        self.worker_id = worker_id\n",
    "        self.action_size = action_size\n",
    "        self.env = DummyGym()\n",
    "        self.gamma = gamma\n",
    "        self.local_model = GlobalModel(action_size)\n",
    "        self.local_model.load_state_dict(self.global_model.state_dict())\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        logits, _ = self.local_model(state)\n",
    "        action_prob = torch.softmax(logits, dim=-1)\n",
    "        action = np.random.choice(self.action_size, p=action_prob.detach().numpy().flatten())\n",
    "        return action\n",
    "\n",
    "    def compute_loss(self, done, state, action, reward, next_state):\n",
    "        logits, value = self.local_model(state)\n",
    "        _, next_value = self.local_model(next_state)\n",
    "        target = reward + (1 - done) * self.gamma * next_value.item()\n",
    "        delta = target - value\n",
    "        policy_loss = -torch.log(torch.softmax(logits, dim=-1)[0, action]) * delta\n",
    "        value_loss = delta ** 2\n",
    "        total_loss = policy_loss + 0.5 * value_loss\n",
    "        return total_loss\n",
    "\n",
    "    def update_global(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for local_param, global_param in zip(self.local_model.parameters(), self.global_model.parameters()):\n",
    "            global_param.grad = local_param.grad  # 同步梯度到全局模型\n",
    "        self.optimizer.step()\n",
    "        self.local_model.load_state_dict(self.global_model.state_dict())  # 同步模型参数\n",
    "\n",
    "    def run(self):\n",
    "        global_episode = 0\n",
    "        while global_episode < num_episodes:\n",
    "            current_state = torch.tensor([self.env.reset()[1]], dtype=torch.float32).unsqueeze(0)\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.choose_action(current_state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = torch.tensor([next_state[1]], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                loss = self.compute_loss(done, current_state, action, reward, next_state)\n",
    "                self.update_global(loss)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                current_state = next_state\n",
    "\n",
    "                if done:\n",
    "                    global_episode += 1\n",
    "                    print(f\"Worker: {self.worker_id}, Episode: {global_episode}, Reward: {episode_reward}\")\n",
    "\n",
    "# 启动多线程 A3C\n",
    "def main():\n",
    "    env = DummyGym()  # 创建一次环境以获取动作空间的大小\n",
    "    action_size = env.action_space.n\n",
    "    global_model = GlobalModel(action_size)\n",
    "    global_model.share_memory()  # 使全局模型在不同线程间共享\n",
    "\n",
    "    optimizer = optim.Adam(global_model.parameters(), lr=learning_rate)\n",
    "    workers = [Worker(global_model, optimizer, action_size, worker_id=i) for i in range(4)]\n",
    "\n",
    "    for worker in workers:\n",
    "        worker.start()\n",
    "    for worker in workers:\n",
    "        worker.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/nuplan/miniconda3/envs/me5418-group10/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n",
      "    exitcode = _main(fd)\n",
      "  File \"/home/nuplan/miniconda3/envs/me5418-group10/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Worker' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/nuplan/miniconda3/envs/me5418-group10/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n",
      "    exitcode = _main(fd)\n",
      "  File \"/home/nuplan/miniconda3/envs/me5418-group10/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Worker' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/nuplan/miniconda3/envs/me5418-group10/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n",
      "    exitcode = _main(fd)\n",
      "  File \"/home/nuplan/miniconda3/envs/me5418-group10/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Worker' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/nuplan/miniconda3/envs/me5418-group10/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n",
      "    exitcode = _main(fd)\n",
      "  File \"/home/nuplan/miniconda3/envs/me5418-group10/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Worker' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)  # 强制使用 'spawn' 方法\n",
    "    except RuntimeError:\n",
    "        pass  # 如果已经设置，跳过此异常\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me5418-group10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
