{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Apply Policy-Gradient to Your Project:\n",
    "\n",
    "1. **Define the Policy Network**: The policy network will take the current state (e.g., observation from the environment) as input and output a probability distribution over actions.\n",
    "2. **Sample Actions**: Based on the policy, the robot will sample an action rather than choosing the one with the highest value.\n",
    "3. **Collect Trajectories**: The agent will interact with the environment, collect states, actions, and rewards.\n",
    "4. **Update the Policy**: After a batch of interactions, the policy will be updated based on the rewards, following the gradient of the expected reward with respect to the policy parameters.\n",
    "5. **Repeat**: The policy is continuously updated as the agent explores the environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Here's how you can integrate this into your `dummy_gym` project:\n",
    "\n",
    "#### 1. **Define the Policy Network**\n",
    "\n",
    "You'll need a neural network that will take the **observation space** (i.e., the car's FOV) and output probabilities for the actions (up, down, left, right).\n",
    "\n",
    "Here’s an example of a simple policy network using **PyTorch**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=-1)  # Output a probability distribution\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **input_size**: The size of the flattened observation space (e.g., FOV matrix).\n",
    "- **output_size**: The number of actions (4 in this case: up, down, left, right).\n",
    "\n",
    "#### 2. **Sample Actions Based on Policy**\n",
    "\n",
    "Once you have the policy network, you sample actions using the action probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_action(policy, state):\n",
    "    state = torch.tensor(state, dtype=torch.float32).flatten()  # Flatten the FOV\n",
    "    action_probs = policy(state)\n",
    "    action_distribution = torch.distributions.Categorical(action_probs)\n",
    "    action = action_distribution.sample()  # Sample an action\n",
    "    return action.item(), action_distribution.log_prob(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. **Collect Trajectories**\n",
    "\n",
    "During each episode, you will interact with the environment and collect states, actions, and rewards:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_trajectory(env, policy, max_steps=1000):\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action, log_prob = select_action(policy, state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return log_probs, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **log_probs**: The log probability of each action taken (for updating the policy).\n",
    "- **rewards**: The rewards received at each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4. **Compute Returns and Policy Gradient Update**\n",
    "\n",
    "Now, use the collected trajectories to compute the returns and update the policy.\n",
    "\n",
    "The **REINFORCE** algorithm maximizes the total expected reward by adjusting the parameters of the policy in the direction of the gradients. Here's how you can implement it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def update_policy(policy, optimizer, log_probs, rewards, gamma=0.99):\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    \n",
    "    loss = 0\n",
    "    for log_prob, R in zip(log_probs, returns):\n",
    "        loss -= log_prob * R  # Minimize the negative log-likelihood\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Gamma**: The discount factor for future rewards.\n",
    "- **Optimizer**: Typically, you use something like Adam (`optim.Adam(policy.parameters(), lr=learning_rate)`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. **Training Loop**\n",
    "\n",
    "Now, integrate the whole process into a training loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dummy_gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-14d2cbaa3498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/ME5418'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdummy_gym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy_gym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDummyGym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_obstacles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFOV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming action_space = 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dummy_gym'"
     ]
    }
   ],
   "source": [
    "\n",
    "import dummy_gym\n",
    "env = dummy_gym.DummyGym(init_pos=(2, 3), map_size=(30, 30), num_of_obstacles=140, FOV=(5, 5))\n",
    "policy = PolicyNetwork(input_size=env.observation_space().size, output_size=4)  # Assuming action_space = 4\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "for episode in range(1000):\n",
    "    log_probs, rewards = collect_trajectory(env, policy)\n",
    "    update_policy(policy, optimizer, log_probs, rewards)\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Steps to Modify Your Environment:\n",
    "\n",
    "1. **State Space**: The state (observation) is the car’s FOV matrix. It should be flattened into a 1D tensor for input into the policy network.\n",
    "2. **Reward Structure**: You already have a reward structure in place, so that can be used to compute the returns.\n",
    "3. **Action Space**: Sample actions based on the policy’s output probabilities.\n",
    "\n",
    "### Summary of Key Concepts:\n",
    "- **Policy Network**: A neural network outputs probabilities for each action.\n",
    "- **Sampling Actions**: Instead of picking the action with the highest probability, you sample from the action distribution.\n",
    "- **Update Policy**: After collecting a batch of trajectories, update the policy using the REINFORCE algorithm, which adjusts the policy based on how much reward each action led to.\n",
    "\n",
    "This framework should help you apply **policy-gradient methods** to your robot exploration project, allowing your agent to learn an exploration strategy through interaction with the environment. If you want to apply more advanced methods like **PPO**, similar steps are involved, but with additional optimizations such as clipping the policy updates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
